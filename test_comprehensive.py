#!/usr/bin/env python3
"""
Comprehensive Testing, Debugging, and Optimization Suite
Tests all aspects of the Agentic Browser Assistant
"""

import time
import json
import traceback
from typing import Dict, List, Tuple
from react_agent_simple import SimpleReActAgent

class ComprehensiveTestSuite:
    """Complete testing suite for the Agentic Browser Assistant."""
    
    def __init__(self):
        """Initialize the test suite."""
        self.agent = None
        self.test_results = []
        self.performance_metrics = {}
        
    def initialize_agent(self) -> bool:
        """Initialize the agent for testing."""
        try:
            print("üöÄ Initializing agent for comprehensive testing...")
            self.agent = SimpleReActAgent(verbose=False)  # Reduce verbosity for cleaner test output
            print("‚úÖ Agent initialized successfully")
            return True
        except Exception as e:
            print(f"‚ùå Agent initialization failed: {e}")
            return False
    
    def run_test_case(self, test_name: str, query: str, expected_behavior: str, timeout: int = 30) -> Dict:
        """Run a single test case with comprehensive metrics."""
        print(f"\nüß™ Running Test: {test_name}")
        print(f"Query: '{query}'")
        print(f"Expected: {expected_behavior}")
        print("-" * 60)
        
        test_result = {
            'name': test_name,
            'query': query,
            'expected': expected_behavior,
            'start_time': time.time(),
            'success': False,
            'response': '',
            'duration': 0,
            'error': None,
            'metrics': {}
        }
        
        try:
            start_time = time.time()
            
            # Run the query with timeout handling
            response = self.agent.query(query)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # Evaluate success based on response content
            success = self._evaluate_response(response, expected_behavior, query)
            
            test_result.update({
                'success': success,
                'response': response,
                'duration': duration,
                'metrics': {
                    'response_length': len(response),
                    'has_sources': 'http' in response.lower(),
                    'has_error': 'error' in response.lower(),
                    'timeout': duration > timeout
                }
            })
            
            status = "‚úÖ PASSED" if success else "‚ö†Ô∏è NEEDS REVIEW"
            print(f"{status} - Duration: {duration:.2f}s - Length: {len(response)} chars")
            
        except Exception as e:
            test_result.update({
                'success': False,
                'error': str(e),
                'duration': time.time() - test_result['start_time']
            })
            print(f"‚ùå FAILED - Error: {e}")
        
        self.test_results.append(test_result)
        return test_result
    
    def _evaluate_response(self, response: str, expected_behavior: str, query: str) -> bool:
        """Evaluate if the response meets expected behavior."""
        response_lower = response.lower()
        query_lower = query.lower()
        
        # Basic quality checks
        if len(response) < 50:
            return False
        
        if "error" in response_lower and "insufficient" in response_lower:
            return False
        
        # Specific behavior checks
        if "weather" in expected_behavior.lower():
            return any(term in response_lower for term in ['weather', 'temperature', 'forecast', 'climate'])
        
        if "compare" in expected_behavior.lower():
            return any(term in response_lower for term in ['compare', 'comparison', 'vs', 'versus', 'difference'])
        
        if "graceful" in expected_behavior.lower():
            return "error" not in response_lower or "alternative" in response_lower
        
        # General content relevance check
        query_words = query_lower.split()
        relevant_words = sum(1 for word in query_words if len(word) > 3 and word in response_lower)
        return relevant_words >= len(query_words) * 0.3
    
    def test_simple_queries(self):
        """Test simple, straightforward queries."""
        print("\n" + "="*70)
        print("üîç SIMPLE QUERY TESTS")
        print("="*70)
        
        simple_tests = [
            {
                'name': 'Weather Query',
                'query': "What's the weather in New York?",
                'expected': 'Should search and summarize weather information'
            },
            {
                'name': 'Definition Query',
                'query': "What is artificial intelligence?",
                'expected': 'Should provide comprehensive definition with examples'
            },
            {
                'name': 'Current Events',
                'query': "What are the latest tech news?",
                'expected': 'Should find and summarize recent technology news'
            }
        ]
        
        for test in simple_tests:
            self.run_test_case(test['name'], test['query'], test['expected'])
    
    def test_complex_queries(self):
        """Test complex queries requiring multiple steps."""
        print("\n" + "="*70)
        print("üîç COMPLEX QUERY TESTS")
        print("="*70)
        
        complex_tests = [
            {
                'name': 'Product Comparison',
                'query': "Compare iPhone 15 vs Galaxy S24",
                'expected': 'Should search, scrape reviews, and provide detailed comparison'
            },
            {
                'name': 'Research Query',
                'query': "Find the best programming languages to learn in 2024 with job market analysis",
                'expected': 'Should gather comprehensive information from multiple sources'
            },
            {
                'name': 'Multi-step Analysis',
                'query': "What are the pros and cons of electric vehicles and their market trends?",
                'expected': 'Should analyze multiple aspects and provide structured response'
            }
        ]
        
        for test in complex_tests:
            self.run_test_case(test['name'], test['query'], test['expected'])
    
    def test_edge_cases(self):
        """Test edge cases and error handling."""
        print("\n" + "="*70)
        print("üîç EDGE CASE TESTS")
        print("="*70)
        
        edge_tests = [
            {
                'name': 'Invalid URL Request',
                'query': "Scrape information from https://invalid-url-that-does-not-exist.com",
                'expected': 'Should handle gracefully with alternative suggestions'
            },
            {
                'name': 'Nonsensical Query',
                'query': "Purple elephant dancing quantum mechanics Tuesday",
                'expected': 'Should handle gracefully and ask for clarification'
            },
            {
                'name': 'Empty-like Query',
                'query': "...",
                'expected': 'Should handle gracefully and provide guidance'
            },
            {
                'name': 'Very Long Query',
                'query': "What is the detailed history of artificial intelligence from its inception in the 1950s through all major milestones including the AI winters, the rise of machine learning, deep learning breakthroughs, and current state-of-the-art models like GPT and their applications in various industries?" * 2,
                'expected': 'Should handle long queries and provide structured response'
            }
        ]
        
        for test in edge_tests:
            self.run_test_case(test['name'], test['query'], test['expected'])
    
    def test_performance_optimization(self):
        """Test performance and optimization features."""
        print("\n" + "="*70)
        print("üîç PERFORMANCE OPTIMIZATION TESTS")
        print("="*70)
        
        # Test response time consistency
        performance_queries = [
            "What is Python programming?",
            "Explain machine learning basics",
            "What are the latest AI developments?"
        ]
        
        response_times = []
        
        for i, query in enumerate(performance_queries, 1):
            print(f"\n‚è±Ô∏è Performance Test {i}/3: '{query}'")
            start_time = time.time()
            
            try:
                response = self.agent.query(query)
                duration = time.time() - start_time
                response_times.append(duration)
                
                print(f"‚úÖ Completed in {duration:.2f}s - Response: {len(response)} chars")
                
            except Exception as e:
                print(f"‚ùå Performance test failed: {e}")
                response_times.append(float('inf'))
        
        # Calculate performance metrics
        if response_times:
            avg_time = sum(t for t in response_times if t != float('inf')) / len([t for t in response_times if t != float('inf')])
            max_time = max(t for t in response_times if t != float('inf'))
            min_time = min(t for t in response_times if t != float('inf'))
            
            self.performance_metrics.update({
                'average_response_time': avg_time,
                'max_response_time': max_time,
                'min_response_time': min_time,
                'response_time_variance': max_time - min_time
            })
            
            print(f"\nüìä Performance Summary:")
            print(f"   Average: {avg_time:.2f}s")
            print(f"   Range: {min_time:.2f}s - {max_time:.2f}s")
            print(f"   Variance: {max_time - min_time:.2f}s")
    
    def test_error_recovery(self):
        """Test error recovery and debugging features."""
        print("\n" + "="*70)
        print("üîç ERROR RECOVERY TESTS")
        print("="*70)
        
        # Test with temporarily broken agent
        original_agent = self.agent
        
        # Test 1: Network simulation (using invalid search)
        print("\nüß™ Testing network error handling...")
        try:
            # This should trigger error handling in tools
            response = self.agent.query("Search for information on a topic that will definitely fail")
            print(f"‚úÖ Network error handled gracefully")
        except Exception as e:
            print(f"‚ö†Ô∏è Network error handling needs improvement: {e}")
        
        # Restore original agent
        self.agent = original_agent
    
    def generate_test_report(self):
        """Generate comprehensive test report."""
        print("\n" + "="*80)
        print("üìä COMPREHENSIVE TEST REPORT")
        print("="*80)
        
        if not self.test_results:
            print("‚ùå No test results available")
            return
        
        # Calculate summary statistics
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['success'])
        failed_tests = total_tests - passed_tests
        
        total_duration = sum(result['duration'] for result in self.test_results)
        avg_duration = total_duration / total_tests if total_tests > 0 else 0
        
        # Overall summary
        print(f"üìà OVERALL RESULTS:")
        print(f"   Total Tests: {total_tests}")
        print(f"   Passed: {passed_tests} ({passed_tests/total_tests*100:.1f}%)")
        print(f"   Failed: {failed_tests} ({failed_tests/total_tests*100:.1f}%)")
        print(f"   Total Time: {total_duration:.2f}s")
        print(f"   Average Time: {avg_duration:.2f}s")
        
        # Performance metrics
        if self.performance_metrics:
            print(f"\n‚ö° PERFORMANCE METRICS:")
            for metric, value in self.performance_metrics.items():
                print(f"   {metric.replace('_', ' ').title()}: {value:.2f}s")
        
        # Detailed results by category
        categories = {}
        for result in self.test_results:
            category = "Simple" if "simple" in result['name'].lower() else \
                      "Complex" if "complex" in result['name'].lower() or "comparison" in result['name'].lower() else \
                      "Edge Case" if any(term in result['name'].lower() for term in ['invalid', 'nonsensical', 'empty', 'long']) else \
                      "Other"
            
            if category not in categories:
                categories[category] = {'passed': 0, 'total': 0, 'avg_time': 0}
            
            categories[category]['total'] += 1
            if result['success']:
                categories[category]['passed'] += 1
            categories[category]['avg_time'] += result['duration']
        
        print(f"\nüìä RESULTS BY CATEGORY:")
        for category, stats in categories.items():
            avg_time = stats['avg_time'] / stats['total'] if stats['total'] > 0 else 0
            pass_rate = stats['passed'] / stats['total'] * 100 if stats['total'] > 0 else 0
            print(f"   {category}: {stats['passed']}/{stats['total']} ({pass_rate:.1f}%) - Avg: {avg_time:.2f}s")
        
        # Failed tests details
        failed_results = [r for r in self.test_results if not r['success']]
        if failed_results:
            print(f"\n‚ùå FAILED TESTS DETAILS:")
            for result in failed_results:
                print(f"   ‚Ä¢ {result['name']}: {result.get('error', 'Response quality issues')}")
        
        # Recommendations
        print(f"\nüí° RECOMMENDATIONS:")
        if avg_duration > 10:
            print("   ‚Ä¢ Consider optimizing response times (current avg: {avg_duration:.2f}s)")
        if failed_tests > 0:
            print("   ‚Ä¢ Review failed test cases for improvement opportunities")
        if self.performance_metrics.get('response_time_variance', 0) > 5:
            print("   ‚Ä¢ Response times show high variance - consider consistency improvements")
        
        print("   ‚Ä¢ All systems are functioning within acceptable parameters ‚úÖ")
        
        return {
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'pass_rate': passed_tests/total_tests*100 if total_tests > 0 else 0,
            'avg_duration': avg_duration,
            'performance_metrics': self.performance_metrics
        }
    
    def run_all_tests(self):
        """Run the complete test suite."""
        print("üöÄ STARTING COMPREHENSIVE TEST SUITE")
        print("="*80)
        print("This will test all aspects of the Agentic Browser Assistant:")
        print("‚Ä¢ Simple queries (basic functionality)")
        print("‚Ä¢ Complex queries (multi-step reasoning)")
        print("‚Ä¢ Edge cases (error handling)")
        print("‚Ä¢ Performance optimization")
        print("‚Ä¢ Error recovery")
        print()
        
        if not self.initialize_agent():
            print("‚ùå Cannot run tests without agent initialization")
            return
        
        # Run all test categories
        try:
            self.test_simple_queries()
            self.test_complex_queries()
            self.test_edge_cases()
            self.test_performance_optimization()
            self.test_error_recovery()
            
            # Generate final report
            report = self.generate_test_report()
            
            print(f"\nüéâ TESTING COMPLETE!")
            print(f"Overall Success Rate: {report['pass_rate']:.1f}%")
            
            return report
            
        except Exception as e:
            print(f"‚ùå Test suite failed with error: {e}")
            traceback.print_exc()
            return None

def main():
    """Run the comprehensive test suite."""
    test_suite = ComprehensiveTestSuite()
    report = test_suite.run_all_tests()
    
    if report and report['pass_rate'] >= 80:
        print("\nüèÜ SYSTEM STATUS: PRODUCTION READY")
        print("The Agentic Browser Assistant passes comprehensive testing!")
    elif report:
        print(f"\n‚ö†Ô∏è SYSTEM STATUS: NEEDS ATTENTION")
        print(f"Pass rate: {report['pass_rate']:.1f}% (target: 80%+)")
    else:
        print("\n‚ùå SYSTEM STATUS: CRITICAL ISSUES")
        print("Please review errors and retry testing.")

if __name__ == "__main__":
    main()